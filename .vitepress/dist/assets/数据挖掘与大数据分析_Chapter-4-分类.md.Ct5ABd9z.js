import{_ as l,c as i,o as t,a1 as a}from"./chunks/framework.BwTyoF0R.js";const e="/assets/Untitled.bgoWJLAk.png",s="/assets/Untitled%201.B7LNRvml.png",n="/assets/Untitled%202.CgmZykpG.png",r="/assets/Untitled%203.CpsryIz1.png",p="/assets/Untitled%204.C_2IMKnp.png",o="/assets/Untitled%205.BAzUrxs4.png",d="/assets/Untitled%206.C_rJIU90.png",u="/assets/Untitled%207.BfyGDnYE.png",h="/assets/Untitled%208.CivAomYZ.png",c="/assets/Untitled%209.umqxHaAX.png",_="/assets/Untitled%2010.BLdw8dRq.png",g="/assets/Untitled%2011.BsJN1TZ6.png",m="/assets/Untitled%2012.C4BAfWHO.png",U="/assets/Untitled%2013.PM0XDhz9.png",b="/assets/Untitled%2014.CKg8zp6L.png",q="/assets/Untitled%2015.DOFKPGw8.png",f="/assets/Untitled%2016.gI_hA18S.png",k="/assets/Untitled%2017.D2mgZebH.png",P="/assets/Untitled%2018.C4HRYvhd.png",x="/assets/Untitled%2019.C339Qh4L.png",C="/assets/Untitled%2020.DxqjXaYL.png",B="/assets/Untitled%2021.HRl3n2-0.png",D="/assets/Untitled%2022.DDcJkVzq.png",T="/assets/Untitled%2023.CnICTaTf.png",M=JSON.parse('{"title":"Chapter-4-分类","description":"","frontmatter":{},"headers":[],"relativePath":"数据挖掘与大数据分析/Chapter-4-分类.md","filePath":"数据挖掘与大数据分析/Chapter-4-分类.md"}'),N={name:"数据挖掘与大数据分析/Chapter-4-分类.md"},v=a('<h1 id="chapter-4-分类" tabindex="-1">Chapter-4-分类 <a class="header-anchor" href="#chapter-4-分类" aria-label="Permalink to &quot;Chapter-4-分类&quot;">​</a></h1><h2 id="一、基本概念" tabindex="-1">一、基本概念 <a class="header-anchor" href="#一、基本概念" aria-label="Permalink to &quot;一、基本概念&quot;">​</a></h2><ol><li><p>监督学习和无监督学习</p><p><strong>无监督学习</strong>：数据集中的对象的类标记是未知的，目的是挖掘潜在的数据内部模式（关联规则挖掘、聚类分析）</p><p><strong>有监督学习</strong>：数据集中对象的类标记已知，通过类标记的指导下学习数据中的模式，利用模式对新数据进行预测。</p><p><img src="'+e+'" alt="这是有监督学习"></p><p>这是有监督学习</p></li><li><p>生成模型和判别模型</p><p><strong>生成模型：</strong></p><ul><li>希望从数据中学习/还原出原始的真实数据生成模型</li><li>常见方法为学习数据的概率分布</li><li>朴素贝叶斯方法、隐马尔可夫模型</li><li>容量大时，更接近生成模型；能处理隐含变量的情景</li></ul><p><strong>判别模型</strong>：</p><ul><li>从数据中学习不同类概念的区别从而进行分类</li><li>KNN、SVM、Decision Tree，etc.</li><li>速度快、准确率较高</li></ul><p><a href="https://zhuanlan.zhihu.com/p/74586507" target="_blank" rel="noreferrer">机器学习中的判别式模型和生成式模型</a></p></li><li><p>分类问题和回归/预测问题</p><p>**分类问题：**数据的分析任务是分类，根据训练集和类标号构建模型</p><p><strong>预测问题</strong>：预测连续值和趋势，建立连续函数值模型，预测未来的情况比如预测空缺值</p></li></ol><h2 id="二、分类" tabindex="-1">二、分类 <a class="header-anchor" href="#二、分类" aria-label="Permalink to &quot;二、分类&quot;">​</a></h2><h3 id="_1-decision-tree" tabindex="-1">1. Decision Tree <a class="header-anchor" href="#_1-decision-tree" aria-label="Permalink to &quot;1. Decision Tree&quot;">​</a></h3><h4 id="基本构造" tabindex="-1">基本构造 <a class="header-anchor" href="#基本构造" aria-label="Permalink to &quot;基本构造&quot;">​</a></h4><ol><li>每个<strong>内部节点</strong>表示一个<strong>属性上的测试</strong></li><li>每个<strong>分支</strong>代表该<strong>测试的输出</strong></li><li>每个<strong>树叶节点</strong>代表<strong>类或类分布</strong></li></ol><p><img src="'+s+'" alt="Untitled"></p><h4 id="算法流程" tabindex="-1">算法流程 <a class="header-anchor" href="#算法流程" aria-label="Permalink to &quot;算法流程&quot;">​</a></h4><p>**基本思想：**贪心算法：自顶向下的分支方式构造决策树（非回溯）</p><p>**三大步骤：**特征选择→决策树生成→剪枝</p><ul><li><p>特征选择</p><ul><li><p>属性选择度量：根据某种启发信息或统计信息进行选择（如：信息增益）,使得分裂子集中待分类项尽可能属于同一类（纯！）</p></li><li><p>信息增益(ID3)</p><ul><li><p>选择具有最高信息增益（即最大熵压缩）的属性作为测试属性</p></li><li><p>计算方式：</p><p><img src="'+n+'" alt="Untitled"></p></li><li><p>Example:</p><p><img src="'+r+'" alt="Untitled"></p></li><li><p>缺点：容易过拟合，即每个划分子集只有一个样本，info(D)=0</p></li></ul></li><li><p>信息增益率：C4.5</p><ul><li>规范化信息增益</li><li>计算方式：</li></ul><p><img src="'+p+'" alt="Untitled"></p></li><li><p>基尼指数：Cart（输出结果必须为二叉树）</p><ul><li>度量数据元组的不纯度。</li><li>计算方式：</li></ul><p><img src="'+o+'" alt="Untitled"></p></li></ul></li><li><p>决策树生成</p><p><img src="'+d+'" alt="Untitled"></p></li></ul><h4 id="算法优缺点" tabindex="-1">算法优缺点 <a class="header-anchor" href="#算法优缺点" aria-label="Permalink to &quot;算法优缺点&quot;">​</a></h4><ul><li>优点 <ol><li>不需要领域知识和参数设置，适合于探测式知识发现</li><li>利于处理高维数据</li><li>可解释性强</li><li>准确度高</li></ol></li><li>缺点 <ol><li>容易过拟合</li><li>忽略了属性间的相关性</li></ol></li></ul><h4 id="过拟合问题" tabindex="-1">过拟合问题 <a class="header-anchor" href="#过拟合问题" aria-label="Permalink to &quot;过拟合问题&quot;">​</a></h4><ul><li>定义：为了得到一致假设而使假设变得过度复杂称为过拟合</li><li>过拟合是监督学习中普遍存在的一个问题 <ul><li>原因：因为训练样本真实情况下的<strong>只是一个抽样集</strong>，数据（测试集和训练集）分布不一致（数据太少，模型太复杂）</li><li>结果：<strong>泛化能力不强</strong></li></ul></li><li>解决策略 <ul><li>增加样本集</li><li>降低模型复杂度</li><li>噪声去除</li><li>Train-Validation-Test【训练（确定模型内部参数）-验证（进行模型的选择）-测试（只使用一次，用于验证最终模型）】</li><li>正则项（Regularization），通过设置惩罚项进行限制（如选择VC维）</li></ul></li></ul><h4 id="树剪枝" tabindex="-1">树剪枝 <a class="header-anchor" href="#树剪枝" aria-label="Permalink to &quot;树剪枝&quot;">​</a></h4><ul><li>决策树中造成过拟合的原因 <ul><li>分枝太多，某些反映训练数据中的异常，噪音/孤立点</li><li>对未参与训练的样本的低精度预测</li></ul></li><li>如何限制 <ul><li>降低层高</li><li>增加叶子节点包含样本最小数（小于该数就停止继续划分）</li><li>树剪枝 <ul><li>先剪枝：提前终止树构造（如果对一个节点的分裂会产生低于给定的阈值的度量，划分停止，阈值选择困难）</li><li>后剪枝：从完全生长的树中剪枝（代价高，对于小样本而言优于先剪枝）</li></ul></li></ul></li></ul><h3 id="_2-knn" tabindex="-1">2. KNN <a class="header-anchor" href="#_2-knn" aria-label="Permalink to &quot;2. KNN&quot;">​</a></h3><h4 id="流程" tabindex="-1">流程 <a class="header-anchor" href="#流程" aria-label="Permalink to &quot;流程&quot;">​</a></h4><ol><li>找距离</li><li>找邻居（最近的k个）</li><li>做测试</li></ol><h4 id="优缺点" tabindex="-1">优缺点 <a class="header-anchor" href="#优缺点" aria-label="Permalink to &quot;优缺点&quot;">​</a></h4><ul><li>优点 <ul><li>简单有效、不需要训练</li><li>在线、多分类/多标签（稀有事件分类）</li></ul></li><li>缺点 <ul><li>懒惰学习：对测试样本分类时计算量大，内存开销大，评分满</li><li>类不平衡：当样本不平衡时，如一个类的样本容量很大，而其他类样本容量很小时，有 可能导致输入一个新样本时，该样本的K个邻居中大容量类的样本占多数；</li><li>可解释性差</li><li>K敏感：K值小分类精度下降，K值大分类效果降低</li></ul></li><li>其他问题 <ul><li>类别的判定方式（距离更近的邻居也许更应该决定最终分类）</li><li>距离度量方式（高维诅咒问题）</li></ul></li></ul><h3 id="_3-朴素贝叶斯-naive-bayes" tabindex="-1">3. 朴素贝叶斯（Naïve Bayes） <a class="header-anchor" href="#_3-朴素贝叶斯-naive-bayes" aria-label="Permalink to &quot;3. 朴素贝叶斯（Naïve Bayes）&quot;">​</a></h3><p><img src="'+u+'" alt="Untitled"></p><ul><li>最终概率和分子呈正相关</li><li>优缺点 <ul><li>优点：输出概率，对文本分类效果较好</li><li>缺点：类条件独立假设（强假设，不符合则降低准确率）</li></ul></li></ul><h3 id="_4-svm" tabindex="-1">4. SVM <a class="header-anchor" href="#_4-svm" aria-label="Permalink to &quot;4. SVM&quot;">​</a></h3><ul><li><p>基本思想：间隔最大化</p></li><li><p>分界上的点称为支持向量</p></li><li><p>优点</p><ul><li>支持小样本学习（支持向量）</li><li>能解决非线性问题（kernel、核技巧）（映射）</li></ul><p><img src="'+h+'" alt="Untitled"></p><ul><li>泛化能力强（结构风险最小化） <ul><li>泛化能力：对未知数据表现良好的能力</li></ul></li></ul></li></ul><h3 id="_5-人工神经网络" tabindex="-1">5. 人工神经网络 <a class="header-anchor" href="#_5-人工神经网络" aria-label="Permalink to &quot;5. 人工神经网络&quot;">​</a></h3><ul><li>理论化的人脑神经网络数学模型，多输入单输出的非线性阈值器件</li><li>输入层、隐藏层、输出层</li></ul><p><img src="'+c+'" alt="Untitled"></p><ul><li>反向传播（通过学习规则，自动调节神经元之间的连接强度或拓扑结构）</li></ul><p><img src="'+_+'" alt="Untitled"></p><ul><li>作用函数、激活函数、传递函数</li></ul><h4 id="_5-1-感知机模型" tabindex="-1">5.1 感知机模型 <a class="header-anchor" href="#_5-1-感知机模型" aria-label="Permalink to &quot;5.1 感知机模型&quot;">​</a></h4><p><img src="'+g+'" alt="Untitled"></p><ul><li>输入、输出用二进制表示。</li><li>基本感知机是一个二层网络，只能解决线性问题</li><li>非线性问题需要用到多层感知机（MLP）</li></ul><p><img src="'+m+'" alt="Untitled"></p><h4 id="_5-2-bp网络" tabindex="-1">5.2 BP网络 <a class="header-anchor" href="#_5-2-bp网络" aria-label="Permalink to &quot;5.2 BP网络&quot;">​</a></h4><p><img src="'+U+'" alt="Untitled"></p><p>连续不断地在相对于误差函数斜率下降的方向上计算网络权值和偏差的变化而逐渐逼近目标的。</p><ul><li>两部分：正向传播、误差的反向传播</li></ul><p><img src="'+b+'" alt="Untitled"></p><ul><li>人工神经网络优缺点 <ul><li>优点：拟合能力强、</li></ul></li><li>缺点 <ul><li>过拟合、训练慢</li></ul></li></ul><h2 id="三、分类评估指标" tabindex="-1">三、分类评估指标 <a class="header-anchor" href="#三、分类评估指标" aria-label="Permalink to &quot;三、分类评估指标&quot;">​</a></h2><ol><li><p>分类准确</p><p><img src="'+q+'" alt="Untitled"></p></li><li><p>非平衡类</p><p><img src="'+f+'" alt="Untitled"></p><p><img src="'+k+'" alt="Untitled"></p></li></ol><h2 id="四、集成学习" tabindex="-1">四、集成学习 <a class="header-anchor" href="#四、集成学习" aria-label="Permalink to &quot;四、集成学习&quot;">​</a></h2><ul><li><p>准则</p><ul><li>基学习器足够好</li><li>基学习器多样化（核心）</li></ul></li><li><p>Bagging</p><p><img src="'+P+'" alt="Untitled"></p><ul><li><p>过程</p><p><img src="'+x+'" alt="取出后需要放回，使得下次采样仍有可能被选中"></p><p>取出后需要放回，使得下次采样仍有可能被选中</p></li></ul></li><li><p>随机森林 (Random Forset，RF) - 引入随机属性选择→(随机选取k个子集，而后在其中选取最优划分)</p></li><li><p>Boosting</p><ul><li><p>将弱学习器提升为强学习器</p><p><img src="'+C+'" alt="Untitled"></p></li><li><p>adaboost</p><p><img src="'+B+'" alt="Untitled"></p></li><li><p>lightGBM</p><p><img src="'+D+'" alt="Untitled"></p></li></ul></li><li><p>Stacking</p><ul><li>先生成一个初学习器，使用它再生成数据用于训练次学习器</li></ul><p><img src="'+T+'" alt="Untitled"></p></li></ul>',48),K=[v];function S(A,V,z,I,R,y){return t(),i("div",null,K)}const H=l(N,[["render",S]]);export{M as __pageData,H as default};
